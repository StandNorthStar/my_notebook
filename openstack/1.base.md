虚拟化:

TLB  虚拟机编号   , 虚拟机切换tag标签避免频繁清写TLB提高TLB缓存命中率

驱动只内核调用.   虚拟机内核和驱动打交道

虚拟机发送报文:

```
   先通过虚拟网卡, 先送给kvm , 然后叫给内核. 处理完成在返回给kvm

   用户空间- 内核

   把驱动程序做成系统调用, vm直接调用.

           明白自己是一个虚拟机, 这种叫半虚拟化\(性能好\) , 跟硬件打交道比较快.

           虚拟机
```

CPU虚拟化:

```
  CPU不支持虚拟化: 模拟特权指令;   模拟

  CPU支持硬件虚拟化: VMM运行ring -1 , 而GuestOS运行在ring 0;  HVM\(硬件辅助\)
```

para-virtualization: 半虚拟化, pv

pv和HVM整合:

```
    os把特权指令直接给vm系统调用 \(hypervisor call 只给虚拟化调用,不给进程.\)

    运行在半虚拟化环境下, 需要修改内核. 所以不能支持Windows

    cpu, io, memroy ;   
    pv on HVM \(不需要前面的CPU虚拟化了, 因为CPU可以自己提供虚拟化指令\)
    IO虚拟化:
       半虚拟化: vm的虚拟网卡- 驱动- 物理驱动  [emulated]
       IO透传技术: vm的虚拟网卡直接使用物理的设备  (有一定的管理接口)
           需要足够的硬件才能使用透传技术.
```

VMM可以理解为操作系统, 只提供硬件虚拟化

qemu - 可以跨平台, 模拟CPU

```
    qemu和xen结合使用. qemu-img
    qemu在用户空间通过软件方式加速实现的.
```

kvm \(kernel-based Virtual Machine基于内核的虚拟机\)虚拟化技术:

```
    linux-
        内核进程
        kvm内核模块\(kvm可以使Linux内核变为hypervisor\)
          所以vm就是一个进程(vm相当于在用户层: guest mode下有用户空间和内核空间)

        CPU虚拟化:kernel分一部分时间片给vm进程
        IO虚拟化(网卡): vm通过vm内核-->vm内核再通过物理机的内核-->硬件

    kvm只支持CPU支持硬件虚拟化技术.必须是x64操作系统.在2.6.20直接整合进kvm内核模块. 在2.6.27后xen也运行进内核
    kvm使用完全虚拟化技术-性能差点. 为了提高性能红帽提供了virtio(pv io),实现了半虚拟化技术. 也支持透传技术()
```

container:

           底层硬件/内核空间/把用户空间隔离了 \(都使用公共的内核\)

           问题: 隔离性\(如果某个容器把内核搞坏了影像到其他容器\)比较差

